import coursier.maven.MavenRepository
import mill._, scalalib._, publish._
import Assembly._
import $ivy.`de.tototec::de.tobiasroeser.mill.vcs.version::0.4.0`
import de.tobiasroeser.mill.vcs.version.VcsVersion

case class SparkVersionSpecific(flag: String, lowerVersion: Option[String] = None, upperVersion: Option[String] = None)

trait SparkModule extends Cross.Module2[String, String] with SbtModule with SonatypeCentralPublishModule {
  outer =>
  override def scalaVersion = crossValue
  val sparkVersion = crossValue2
  val Array(sparkMajor, sparkMinor, sparkPatch) = sparkVersion.split("\\.")
  val sparkBinaryVersion = s"$sparkMajor.$sparkMinor"

  // Set Java target version based on Spark version requirements
  override def javacOptions = T {
    if (sparkVersion >= "4.0.0") {
      Seq("-target", "17", "-source", "17")
    } else {
      Seq("-target", "11", "-source", "11")
    }
  }

  override def millSourcePath = super.millSourcePath / os.up

  override def scalacPluginIvyDeps = T {
    super.scalacPluginIvyDeps() ++ Agg(ivy"com.eed3si9n.ifdef::ifdef-plugin:0.4.1")
  }

  override def scalacOptions = T {
    val baseOptions = super.scalacOptions()

    val sparkVersionSpecificFlags = Seq(
      // Date/Time handling
      SparkVersionSpecific("legacyTimestampStringCleaning", lowerVersion = Some("3.0.0"), upperVersion = Some("3.1.0")),
      SparkVersionSpecific("stringToDateZoneIdSupport", upperVersion = Some("3.2.0")),
      SparkVersionSpecific("dateFormatter:WithZoneId", lowerVersion = Some("3.0.0"), upperVersion = Some("3.2.0")),

      // Filter handling
      SparkVersionSpecific("filterHandling:CsvBased", upperVersion = Some("3.1.0")),
      SparkVersionSpecific("filterHandling:Structured", lowerVersion = Some("3.1.0")),
      SparkVersionSpecific("catalystFilterPushdown", lowerVersion = Some("3.3.0")),
      SparkVersionSpecific("withFilters", lowerVersion = Some("3.0.0"), upperVersion = Some("3.3.0")),

      // File handling
      SparkVersionSpecific("useFileSourceOptions", lowerVersion = Some("3.4.0")),
      SparkVersionSpecific("hasFileSourceOptionsMethod", lowerVersion = Some("3.4.2")),

      // Write API
      SparkVersionSpecific("writebuilderReturnsWrite", lowerVersion = Some("3.2.0")),
      SparkVersionSpecific("sparkFileWriteTrait", lowerVersion = Some("3.2.0")),
      SparkVersionSpecific("exposedPathProperty", lowerVersion = Some("3.2.0")),

      // Breaking changes
      SparkVersionSpecific("badRecordPartialResultsOption", upperVersion = Some("3.4.2"))
    )

    def versionMatches(spec: SparkVersionSpecific): Boolean = {
      val lowerMatch = spec.lowerVersion.forall(sparkVersion >= _)
      val upperMatch = spec.upperVersion.forall(sparkVersion < _)
      lowerMatch && upperMatch
    }

    val featureFlags = sparkVersionSpecificFlags.filter(versionMatches).map(_.flag)

    val allFlags = featureFlags.map(flag =>
      if (scalaVersion().startsWith("2.")) s"com.eed3si9n.ifdef.declare:$flag"
      else s"com.eed3si9n.ifdef.declare:$flag"
    )
    val ifdefOptions = if (scalaVersion().startsWith("2.")) {
      allFlags.flatMap(flag => Seq("-Xmacro-settings", flag))
    } else {
      allFlags.map(flag => s"-Xmacro-settings:$flag")
    }

    baseOptions ++ ifdefOptions
  }

  override def docSources = T.sources(Seq[PathRef]())

  override def artifactName = "spark-excel"

  override def publishVersion: T[String] = T {
    val vcsVersion = VcsVersion.vcsState().format(untaggedSuffix = "-SNAPSHOT")
    s"${sparkVersion}_${vcsVersion}"
  }
  def pomSettings = PomSettings(
    description = "A Spark plugin for reading and writing Excel files",
    organization = "dev.mauch",
    url = "https://github.com/nightscape/spark-excel",
    licenses = Seq(License.`Apache-2.0`),
    versionControl = VersionControl.github("nightscape", "spark-excel"),
    developers = Seq(Developer("nightscape", "Martin Mauch", "https://github.com/nightscape"))
  )

  def assemblyRules = Seq(
    Rule.AppendPattern(".*\\.conf"), // all *.conf files will be concatenated into single file
    Rule.Relocate("org.apache.commons.io.**", "shadeio.commons.io.@1"),
    Rule.Relocate("org.apache.commons.compress.**", "shadeio.commons.compress.@1")
  )

  override def extraPublish = Seq(
    PublishInfo(assembly(), classifier = None, ivyConfig = "compile"),
    PublishInfo(jar(), classifier = Some("thin"), ivyConfig = "compile")
  )

  override def sonatypeCentralReadTimeout: T[Int] = 600000
  override def sonatypeCentralAwaitTimeout: T[Int] = 1200 * 1000

  val sparkDeps = Agg(
    ivy"org.apache.spark::spark-core:$sparkVersion",
    ivy"org.apache.spark::spark-sql:$sparkVersion",
    ivy"org.apache.spark::spark-hive:$sparkVersion"
  )

  override def compileIvyDeps = if (sparkVersion < "3.3.0") {
    sparkDeps ++ Agg(ivy"org.slf4j:slf4j-api:1.7.36".excludeOrg("stax"))
  } else {
    sparkDeps
  }

  val poiVersion = "5.4.1"

  override def ivyDeps = {
    val base = Agg(
      ivy"org.apache.poi:poi:$poiVersion",
      ivy"org.apache.poi:poi-ooxml:$poiVersion",
      ivy"org.apache.poi:poi-ooxml-lite:$poiVersion",
      ivy"org.apache.xmlbeans:xmlbeans:5.3.0",
      ivy"com.norbitltd::spoiwo:2.2.1",
      ivy"com.github.pjfanning:excel-streaming-reader:5.1.1",
      ivy"commons-io:commons-io:2.20.0",
      ivy"org.apache.commons:commons-compress:1.27.1",
      ivy"org.apache.logging.log4j:log4j-api:2.24.3",
      ivy"com.zaxxer:SparseBitSet:1.3",
      ivy"org.apache.commons:commons-collections4:4.5.0",
      ivy"com.github.virtuald:curvesapi:1.08",
      ivy"commons-codec:commons-codec:1.19.0",
      ivy"org.apache.commons:commons-math3:3.6.1",
      ivy"org.scala-lang.modules::scala-collection-compat:2.13.0",
      ivy"com.eed3si9n.ifdef::ifdef-annotation:0.4.1"
    )
    if (sparkVersion >= "3.3.0") {
      base ++ Agg(ivy"org.apache.logging.log4j:log4j-core:2.24.3")
    } else {
      base
    }
  }

  object test extends SbtTests with TestModule.ScalaTest {

    override def millSourcePath = super.millSourcePath

    override def resources = T.sources {
      Seq(PathRef(millSourcePath / "src" / "test" / "resources"))
    }

    def scalaVersion = outer.scalaVersion()

    // Add JVM options to handle Java module restrictions (JAVA 11+)
    override def forkArgs = T {
      Seq(
        "-XX:+IgnoreUnrecognizedVMOptions",
        "--add-exports",
        "java.base/sun.nio.ch=ALL-UNNAMED",
        "--add-exports",
        "java.base/sun.util.calendar=ALL-UNNAMED",
        "--add-opens",
        "java.base/java.lang=ALL-UNNAMED",
        "--add-opens",
        "java.base/java.net=ALL-UNNAMED"
      )
    }

    def repositoriesTask = T.task {
      super.repositoriesTask() ++ Seq(MavenRepository("https://jitpack.io"))
    }

    def ivyDeps = sparkDeps ++ Agg(
      ivy"org.typelevel::cats-core:2.13.0",
      ivy"org.scalatest::scalatest:3.2.19",
      ivy"org.scalatestplus::scalacheck-1-16:3.2.14.0",
      ivy"org.scalacheck::scalacheck:1.18.1",
      ivy"com.github.alexarchambault::scalacheck-shapeless_1.15:1.3.0",
      ivy"com.github.mrpowers::spark-fast-tests:3.0.1",
      ivy"org.scalamock::scalamock:5.2.0"
    )
  }

}

val scala213 = "2.13.16"
val scala212 = "2.12.20"
val spark30 = List("3.0.3")
val spark31 = List("3.1.3")
val spark32 = List("3.2.4")
val spark33 = List("3.3.4")
val spark34 = List("3.4.4", "3.4.1")
val spark35 = List("3.5.6")
val spark40 = List("4.0.0")
val sparkVersions = spark30 ++ spark31 ++ spark32 ++ spark33 ++ spark34 ++ spark35 ++ spark40
val crossMatrix =
  sparkVersions.filter(_ < "4.0").map(spark => (scala212, spark)) ++
    sparkVersions.filter(_ >= "3.2").map(spark => (scala213, spark))

object `spark-excel` extends Cross[SparkModule](crossMatrix) {}

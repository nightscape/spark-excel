import coursier.maven.MavenRepository
import mill._, scalalib._, publish._
import Assembly._
import $ivy.`de.tototec::de.tobiasroeser.mill.vcs.version::0.4.0`
import de.tobiasroeser.mill.vcs.version.VcsVersion

trait SparkModule extends Cross.Module2[String, String] with SbtModule with SonatypeCentralPublishModule {
  outer =>
  override def scalaVersion = crossValue
  val sparkVersion = crossValue2
  val Array(sparkMajor, sparkMinor, sparkPatch) = sparkVersion.split("\\.")
  val sparkBinaryVersion = s"$sparkMajor.$sparkMinor"
  
  // Set Java target version based on Spark version requirements
  override def javacOptions = T {
    if (sparkVersion >= "4.0.0") {
      Seq("-target", "17", "-source", "17")
    } else {
      Seq("-target", "11", "-source", "11")  
    }
  }

  override def millSourcePath = super.millSourcePath / os.up

  object LowerOrEqual {
    def unapply(otherVersion: String): Boolean = otherVersion match {
      case s"${sparkMaj}.${sparkMin}.${sparkPat}" =>
        (sparkMaj.toInt < sparkMajor.toInt) || 
        (sparkMaj == sparkMajor && (sparkMin.toInt < sparkMinor.toInt || (sparkMin == sparkMinor && sparkPat.toInt <= sparkPatch.toInt)))
      case s"${sparkMaj}.${sparkMin}" => 
        (sparkMaj.toInt < sparkMajor.toInt) || 
        (sparkMaj == sparkMajor && sparkMin.toInt <= sparkMinor.toInt)
      case sparkMaj => sparkMaj.toInt <= sparkMajor.toInt
    }
  }
  object HigherOrEqual {
    def unapply(otherVersion: String): Boolean = otherVersion match {
      case s"${sparkMaj}.${sparkMin}.${sparkPat}" =>
        (sparkMaj.toInt > sparkMajor.toInt) || 
        (sparkMaj == sparkMajor && (sparkMin.toInt > sparkMinor.toInt || (sparkMin == sparkMinor && sparkPat.toInt >= sparkPatch.toInt)))
      case s"${sparkMaj}.${sparkMin}" => 
        (sparkMaj.toInt > sparkMajor.toInt) || 
        (sparkMaj == sparkMajor && sparkMin.toInt >= sparkMinor.toInt)
      case sparkMaj => sparkMaj.toInt >= sparkMajor.toInt
    }
  }

  def sparkVersionSpecificSources = T {
    val versionSpecificDirs = os.list(mill.api.WorkspaceRoot.workspaceRoot / "src" / "main")
    val Array(sparkMajor, sparkMinor, sparkPatch) = sparkVersion.split("\\.")
    val sparkBinaryVersion = s"$sparkMajor.$sparkMinor"
    
    // For Spark 4.0+, prefer 4.0_and_up over older version ranges
    val matchingDirs = versionSpecificDirs.filter(_.last match {
      case "scala" => true
      case `sparkBinaryVersion` => true
      case s"${LowerOrEqual()}_and_up" => true
      case s"${LowerOrEqual()}_to_${HigherOrEqual()}" => true
      case _ => false
    })
    
    // Return all matching directories - let version-specific files be prioritized by Mill's source ordering
    matchingDirs
  }
  override def sources = T.sources {
    super.sources() ++ sparkVersionSpecificSources().map(PathRef(_))
  }

  override def docSources = T.sources(Seq[PathRef]())

  override def artifactName = "spark-excel"

  override def publishVersion: T[String] = T {
    val vcsVersion = VcsVersion.vcsState().format(untaggedSuffix = "-SNAPSHOT")
    s"${sparkVersion}_${vcsVersion}"
  }
  def pomSettings = PomSettings(
    description = "A Spark plugin for reading and writing Excel files",
    organization = "dev.mauch",
    url = "https://github.com/nightscape/spark-excel",
    licenses = Seq(License.`Apache-2.0`),
    versionControl = VersionControl.github("nightscape", "spark-excel"),
    developers = Seq(Developer("nightscape", "Martin Mauch", "https://github.com/nightscape"))
  )

  def assemblyRules = Seq(
    Rule.AppendPattern(".*\\.conf"), // all *.conf files will be concatenated into single file
    Rule.Relocate("org.apache.commons.io.**", "shadeio.commons.io.@1"),
    Rule.Relocate("org.apache.commons.compress.**", "shadeio.commons.compress.@1")
  )

  override def extraPublish = Seq(
    PublishInfo(assembly(), classifier = None, ivyConfig = "compile"),
    PublishInfo(jar(), classifier = Some("thin"), ivyConfig = "compile")
  )

  override def sonatypeCentralReadTimeout: T[Int] = 600000
  override def sonatypeCentralAwaitTimeout: T[Int] = 1200 * 1000

  val sparkDeps = Agg(
    ivy"org.apache.spark::spark-core:$sparkVersion",
    ivy"org.apache.spark::spark-sql:$sparkVersion",
    ivy"org.apache.spark::spark-hive:$sparkVersion"
  )

  override def compileIvyDeps = if (sparkVersion < "3.3.0") {
    sparkDeps ++ Agg(ivy"org.slf4j:slf4j-api:1.7.36".excludeOrg("stax"))
  } else {
    sparkDeps
  }

  val poiVersion = "5.4.1"

  override def ivyDeps = {
    val base = Agg(
      ivy"org.apache.poi:poi:$poiVersion",
      ivy"org.apache.poi:poi-ooxml:$poiVersion",
      ivy"org.apache.poi:poi-ooxml-lite:$poiVersion",
      ivy"org.apache.xmlbeans:xmlbeans:5.3.0",
      ivy"com.norbitltd::spoiwo:2.2.1",
      ivy"com.github.pjfanning:excel-streaming-reader:5.1.0",
      ivy"commons-io:commons-io:2.19.0",
      ivy"org.apache.commons:commons-compress:1.27.1",
      ivy"org.apache.logging.log4j:log4j-api:2.24.3",
      ivy"com.zaxxer:SparseBitSet:1.3",
      ivy"org.apache.commons:commons-collections4:4.5.0",
      ivy"com.github.virtuald:curvesapi:1.08",
      ivy"commons-codec:commons-codec:1.18.0",
      ivy"org.apache.commons:commons-math3:3.6.1",
      ivy"org.scala-lang.modules::scala-collection-compat:2.13.0"
    )
    if (sparkVersion >= "3.3.0") {
      base ++ Agg(ivy"org.apache.logging.log4j:log4j-core:2.24.3")
    } else {
      base
    }
  }

  object test extends SbtTests with TestModule.ScalaTest {

    override def millSourcePath = super.millSourcePath

    override def resources = T.sources {
      Seq(PathRef(millSourcePath / "src" / "test" / "resources"))
    }

    def scalaVersion = outer.scalaVersion()

    // Add JVM options for Spark 3.0+ to handle Java module restrictions (JAVA 11+)
    override def forkArgs = T {
      if (sparkVersion >= "3.0.0") {
        Seq(
          "-XX:+IgnoreUnrecognizedVMOptions",
          "--add-exports", "java.base/sun.nio.ch=ALL-UNNAMED",
          "--add-exports", "java.base/sun.util.calendar=ALL-UNNAMED",
          "--add-opens", "java.base/java.lang=ALL-UNNAMED",
          "--add-opens", "java.base/java.net=ALL-UNNAMED"
        )
      } else {
        Seq()
      }
    }

    def repositoriesTask = T.task {
      super.repositoriesTask() ++ Seq(MavenRepository("https://jitpack.io"))
    }

    def ivyDeps = sparkDeps ++ Agg(
      ivy"org.typelevel::cats-core:2.13.0",
      ivy"org.scalatest::scalatest:3.2.19",
      ivy"org.scalatestplus::scalacheck-1-16:3.2.14.0",
      ivy"org.scalacheck::scalacheck:1.18.1",
      ivy"com.github.alexarchambault::scalacheck-shapeless_1.15:1.3.0",
      ivy"com.github.mrpowers::spark-fast-tests:3.0.1",
      ivy"org.scalamock::scalamock:5.2.0"
    )
  }

}

val scala213 = "2.13.16"
val scala212 = "2.12.20"
val spark24 = List("2.4.8")
val spark30 = List("3.0.3")
val spark31 = List("3.1.3")
val spark32 = List("3.2.4")
val spark33 = List("3.3.4")
val spark34 = List("3.4.4", "3.4.1")
val spark35 = List("3.5.6")
val spark40 = List("4.0.0")
val sparkVersions = spark24 ++ spark30 ++ spark31 ++ spark32 ++ spark33 ++ spark34 ++ spark35 ++ spark40
val crossMatrix =
  sparkVersions.filter(_ < "4.0").map(spark => (scala212, spark)) ++
    sparkVersions.filter(_ >= "3.2").map(spark => (scala213, spark))

object `spark-excel` extends Cross[SparkModule](crossMatrix) {}
